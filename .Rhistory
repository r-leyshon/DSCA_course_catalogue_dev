#generalised function for generating urls to scrape search result pages
all_pages <- generate_urls(url_suffix)
paste("Page URLs to scrape links from: ", all_pages)
#extract all course repo names from all_pages url vector
course_repo_names <- lapply(all_pages, extract_repo_names) %>% unlist()
#need a generalised function to use in lapply.
# verification messages to console
print(
paste("There are", length(course_repo_names), "courses found. Here are the repo names: ")
)
#messages to console
print(paste(course_repo_names))
View(extract_course_description)
#purpose of script:
"Scrape data from online github DSC repo. This script handles course titles"
source('functions/functions.R')
#search result used, parameter to specify contains 'DSCA' but not 'dev'
search_result <- "https://github.com/search?q=org%3Adatasciencecampus+DSCA++NOT+_dev&type=Repositories" #public repos
url_prefix <- 'https://github.com/search?p='
url_suffix <- '&q=org%3Adatasciencecampus+DSCA++NOT+_dev&type=Repositories' # public repos
url_suffix <- '&q=user%3Ahadley+r&type=Repositories' #test on Hadley Wickham R repos.
#6 pages with 57 repos (all appear open) at this time.
#use function to extract the last page from the search result as numeric
last_page <- get_last_page(search_result)
"Hard code last_page for time being when testing on other repos"
last_page <- 7
print(paste("Total number of pages in search result: ", last_page))
#generalised function for generating urls to scrape search result pages
all_pages <- generate_urls(url_suffix)
paste("Page URLs to scrape links from: ", all_pages)
#extract all course repo names from all_pages url vector
course_repo_names <- lapply(all_pages, extract_repo_names) %>% unlist()
#need a generalised function to use in lapply.
# verification messages to console
print(
paste("There are", length(course_repo_names), "courses found. Here are the repo names: ")
)
#messages to console
print(paste(course_repo_names))
username <- "hadley"
sprintf('&q=user%3%s+r&type=Repositories', username)
sprintf('&q=user%3 %s +r&type=Repositories', username)
source('H:/My Documents/R/Faculty Projects/course catalogue/workflow/src/initialise.R', echo=TRUE)
remove(version_number)
#use function to extract the last page from the search result as numeric
last_page <- get_last_page(search_result)
last_page
search_result_links <- search_result %>% read_html() %>% html_nodes("a") %>% html_text()
search_result_links
source('H:/My Documents/R/Faculty Projects/course catalogue/workflow/src/initialise.R', echo=TRUE)
source('H:/My Documents/R/Faculty Projects/course catalogue/workflow/src/initialise.R', echo=TRUE)
source('H:/My Documents/R/Faculty Projects/course catalogue/workflow/src/initialise.R', echo=TRUE)
View(output_dataframe)
source('functions/functions.R')
#search result used, parameter to specify contains 'DSCA' but not 'dev'
search_result <- "https://github.com/search?q=org%3Adatasciencecampus+DSCA++NOT+_dev&type=Repositories" #public repos
search_result <- "https://github.com/search?q=org%3Adatasciencecampus+dev&type=Repositories"
url_prefix <- 'https://github.com/search?p='
url_suffix <- '&q=org%3Adatasciencecampus+DSCA++NOT+_dev&type=Repositories' # public repos
url_suffix <- '&q=org%3Adatasciencecampus+dev&type=Repositories&unscoped_q=dev'
#use function to extract the last page from the search result as numeric
last_page <- get_last_page(search_result)
"hard code all_pages object until generalised method identified"
last_page <- 2
print(paste("Total number of pages in search result: ", last_page))
# #generalised function for generating urls to scrape search result pages
all_pages <- generate_urls(url_suffix)
paste("Page URLs to scrape links from: ", all_pages)
#extract all course repo names from all_pages url vector
course_repo_names <- lapply(all_pages, extract_repo_names) %>% unlist()
course_repo_names
all_pages
paste("Page URLs to scrape links from: ", all_pages)
course_repo_names
hrefs <-  all_pages %>% read_html() %>%
#extract all 'a' nodes
html_nodes("a") %>%
#extract all href links
html_attr('href')
hrefs <-  all_pages[1] %>% read_html() %>%
#extract all 'a' nodes
html_nodes("a") %>%
#extract all href links
html_attr('href')
hrefs
#purpose of script:
"Scrape data from online github DSC repo. This script handles course titles"
source('functions/functions.R')
#search result used, parameter to specify contains 'DSCA' but not 'dev'
search_result <- "https://github.com/search?q=org%3Adatasciencecampus+DSCA++NOT+_dev&type=Repositories" #public repos
#search_result <- "https://github.com/search?q=org%3Adatasciencecampus+dev&type=Repositories"
url_prefix <- 'https://github.com/search?p='
url_suffix <- '&q=org%3Adatasciencecampus+DSCA++NOT+_dev&type=Repositories' # public repos
#url_suffix <- '&q=org%3Adatasciencecampus+dev&type=Repositories&unscoped_q=dev'
#use function to extract the last page from the search result as numeric
last_page <- get_last_page(search_result)
"hard code all_pages object until generalised method identified"
last_page <- 2
print(paste("Total number of pages in search result: ", last_page))
# #generalised function for generating urls to scrape search result pages
all_pages <- generate_urls(url_suffix)
paste("Page URL to scrape links from: ", all_pages)
hrefs <-  all_pages[1] %>% read_html() %>%
#extract all 'a' nodes
html_nodes("a") %>%
#extract all href links
html_attr('href')
hrefs
source('H:/My Documents/R/Faculty Projects/course catalogue/workflow/src/initialise.R', echo=TRUE)
View(output_dataframe)
paste("There is no description available for: ",
output_dataframe$course_repo_names[is.na(output_dataframe[, 5])]
)
print(paste("There is no description available for: ",
output_dataframe$course_repo_names[is.na(output_dataframe[, 5])]
))
source('H:/My Documents/R/Faculty Projects/course catalogue/workflow/src/initialise.R', echo=TRUE)
print(paste("There is no description available for: ",
output_dataframe$course_repo_names[is.na(output_dataframe[, 4])]
))
is.na(is.na(output_dataframe[, 5]))
print(paste("There is no description available for: ",
output_dataframe$course_repo_names[is.na(output_dataframe[, 5])]
))
is.na(is.na(output_dataframe[, 5]))
is.na(is.na(output_dataframe[, 6]))
is.na(is.na(output_dataframe[, 19]))
is.na(output_dataframe[, 19])
is.na(output_dataframe[, 5])
any( is.na(output_dataframe[, 5]))
any( is.na(output_dataframe[, 4]))
any( is.na(output_dataframe[, 5]))
if (any(is.na(output_dataframe[, 5]))) {
print(paste("There is no description available for: ",
output_dataframe$course_repo_names[is.na(output_dataframe[, 5])]
))
}
if (any(is.na(output_dataframe[, 5]))) {
print(paste("There is no description available for: ",
output_dataframe$course_repo_names[is.na(output_dataframe[, 4])]
))
}
if (any(is.na(output_dataframe[, 4]))) {
print(paste("There is no description available for: ",
output_dataframe$course_repo_names[is.na(output_dataframe[, 5])]
))
}
if (any(is.na(output_dataframe[, 21]))) {
print(paste("There is no description available for: ",
output_dataframe$course_repo_names[is.na(output_dataframe[, 5])]
))
}
if (any(is.na(output_dataframe[, 5]))) {
print(paste("There is no description available for: ",
output_dataframe$course_repo_names[is.na(output_dataframe[, 5])]
))
}
source('H:/My Documents/R/Faculty Projects/course catalogue/workflow/src/initialise.R', echo=TRUE)
################working here######################
################working here######################
################working here######################
#need a generalised method for identifying last page in search result
test_url <- 'https://github.com/search?q=org%3Adatasciencecampus+DSCA++NOT+_dev&type=Repositories'
read_html(test_url)
read_html(test_url) %>% html_nodes(".next_page")
read_html(test_url) %>% html_nodes(".next_page") %>% html_text()
################working here######################
################working here######################
################working here######################
#need a generalised method for identifying last page in search result
test_url <- 'https://github.com/search?p=1&q=org%3Adatasciencecampus+DSCA++NOT+_dev&type=Repositories'
read_html(test_url) %>% html_nodes(".next_page") %>% html_text()
test_url <- 'https://github.com/search?p=2&q=org%3Adatasciencecampus+DSCA++NOT+_dev&type=Repositories'
read_html(test_url) %>% html_nodes(".next_page") %>% html_text()
test_url
read_html(test_url) %>% html_nodes(".next_page")
read_html(test_url) %>% html_nodes(".next_page") %>% html_attr()
read_html(test_url) %>% html_nodes(".next_page") %>% html_attrs()
################working here######################
################working here######################
################working here######################
#need a generalised method for identifying last page in search result
test_url <- 'https://github.com/search?p=1&q=org%3Adatasciencecampus+DSCA++NOT+_dev&type=Repositories'
read_html(test_url) %>% html_nodes(".next_page") %>% html_attrs()
read_html(test_url) %>% html_nodes(".next_page") %>% html_attrs() %>% str()
read_html(test_url) %>% html_nodes(".next_page") %>% html_attrs() %>% class()
read_html(test_url) %>% html_nodes(".next_page") %>% html_attrs() %>% unlist()
test_url <- 'https://github.com/search?p=2&q=org%3Adatasciencecampus+DSCA++NOT+_dev&type=Repositories'
read_html(test_url) %>% html_nodes(".next_page") %>% html_attrs() %>% unlist()
read_html(test_url) %>% html_nodes(".next_page") %>% html_attrs() %>% unlist() %>% str()
read_html(test_url) %>% html_nodes(".next_page") %>% html_attrs() %>% unlist() %>% class()
read_html(test_url) %>% html_nodes(".next_page") %>% html_attrs() %>% unlist() %>% class() %>% grep("disabled")
read_html(test_url) %>% html_nodes(".next_page") %>% html_attrs() %>% unlist() %>% class() %>% grepl("disabled")
test_url <- 'https://github.com/search?p=2&q=org%3Adatasciencecampus+DSCA++NOT+_dev&type=Repositories'
read_html(test_url) %>% html_nodes(".next_page") %>% html_attrs() %>% unlist() %>% class() %>% grepl("disabled")
read_html(test_url) %>% html_nodes(".next_page") %>% html_attrs() %>% unlist() %>% grepl("disabled")
read_html(test_url) %>% html_nodes(".next_page") %>% html_attrs() %>% unlist() %>% grep("disabled")
read_html(test_url) %>% html_nodes(".next_page") %>% html_attrs() %>% unlist()
read_html(test_url) %>% html_nodes(".next_page") %>% html_attrs() %>% unlist() %>% grepl("e")
read_html(test_url) %>% html_nodes(".next_page") %>% html_attrs() %>% unlist() %>% grepl("a")
read_html(test_url) %>% html_nodes(".next_page") %>% html_attrs() %>% unname()
read_html(test_url) %>% html_nodes(".next_page") %>% html_attrs() %>% unname() class()
read_html(test_url) %>% html_nodes(".next_page") %>% html_attrs() %>% unname() %>% class()
read_html(test_url) %>% html_nodes(".next_page") %>% html_attrs() %>% unname() %>% unlist()
read_html(test_url) %>% html_nodes(".next_page") %>% html_attrs() %>% unname() %>% unlist() %>% class()
read_html(test_url) %>% html_nodes(".next_page") %>% html_attrs() %>% unname() %>% unlist() %>% grepl("disabled")
read_html(test_url) %>% html_nodes(".next_page") %>% html_attrs() %>% unname() %>% unlist() %>% str_detect("disabled")
read_html(test_url) %>%
html_nodes(".next_page") %>%
html_attrs() %>%
#unname() %>%
unlist() %>%
str_detect("disabled")
read_html(test_url) %>%
html_nodes(".next_page") %>%
html_attrs() %>%
#unname() %>%
#unlist() %>%
str_detect("disabled")
################working here######################
################working here######################
################working here######################
#need a generalised method for identifying last page in search result
test_url <- 'https://github.com/search?p=1&q=org%3Adatasciencecampus+DSCA++NOT+_dev&type=Repositories'
read_html(test_url) %>%
html_nodes(".next_page") %>%
html_attrs() %>%
str_detect("disabled")
read_html(test_url) %>%
html_nodes(".next_page") %>%
html_attrs() %>%
unlist() %>%
str_detect("disabled")
test_url <- 'https://github.com/search?p=2&q=org%3Adatasciencecampus+DSCA++NOT+_dev&type=Repositories'
read_html(test_url) %>%
html_nodes(".next_page") %>%
html_attrs() %>%
unlist() %>%
str_detect("disabled")
################working here######################
################working here######################
################working here######################
#need a generalised method for identifying last page in search result
test_url <- 'https://github.com/search?p=1&q=org%3Adatasciencecampus+DSCA++NOT+_dev&type=Repositories'
read_html(test_url) %>%
html_nodes(".next_page") %>%
html_attrs() %>%
unlist() %>%
str_detect("disabled")
read_html(test_url) %>%
html_nodes(".next_page") %>%
html_attrs() %>%
unlist() %>%
any(str_detect("disabled"))
read_html(test_url) %>%
html_nodes(".next_page") %>%
html_attrs() %>%
unlist() %>%
str_detect("disabled")
read_html(test_url) %>%
html_nodes(".next_page") %>%
html_attrs() %>%
unlist() %>%
str_detect("disabled") %>%
any()
test_url <- 'https://github.com/search?p=2&q=org%3Adatasciencecampus+DSCA++NOT+_dev&type=Repositories'
read_html(test_url) %>%
html_nodes(".next_page") %>%
html_attrs() %>%
unlist() %>%
str_detect("disabled") %>%
any()
check_urls <- str_c(url_prefix, 1:100, url_suffix)
url_prefix <- 'https://github.com/search?p='
url_suffix <- '&q=org%3Adatasciencecampus+DSCA++NOT+_dev&type=Repositories' # public repos
check_urls <- str_c(url_prefix, 1:100, url_suffix)
check_urls
read_html(check_urls) %>%
html_nodes(".next_page") %>%
html_attrs() %>%
unlist() %>%
str_detect("disabled") %>%
any()
read_html(check_urls[1]) %>%
html_nodes(".next_page") %>%
html_attrs() %>%
unlist() %>%
str_detect("disabled") %>%
any()
read_html(check_urls[2]) %>%
html_nodes(".next_page") %>%
html_attrs() %>%
unlist() %>%
str_detect("disabled") %>%
any()
read_html(check_urls[3]) %>%
html_nodes(".next_page") %>%
html_attrs() %>%
unlist() %>%
str_detect("disabled") %>%
any()
read_html(check_urls[4]) %>%
html_nodes(".next_page") %>%
html_attrs() %>%
unlist() %>%
str_detect("disabled") %>%
any()
get_last_page <- function(urls_to_check){
read_html(urls_to_check) %>%
html_nodes(".next_page") %>%
html_attrs() %>%
unlist() %>%
str_detect("disabled") %>%
any()
}
get_last_page(check_urls)
get_last_page(check_urls[1])
get_last_page(check_urls[2])
lapply(check_urls, get_last_page)
for pages in check_urls {
print(pages)
}
for pages in check_urls {
print(check_urls[n])
}
for pages in check_urls
print(check_urls[n])
for (pages in check_urls) {
print(pages)
}
for (pages in check_urls) {
get_last_page(pages)
}
check_urls <- str_c(url_prefix, 1:10, url_suffix)
for (pages in check_urls) {
get_last_page(pages)
}
for (pages in check_urls) {
read_html(pages)
}
while (get_last_page(check_urls) == TRUE) {
print(pages)
}
while (get_last_page(check_urls[1]) == TRUE) {
print(pages)
}
}
}
}
{get_last_page(check_urls[1]}
{print(1)}
{print(1)}
{print(1)}
get_last_page(check_urls[3]
get_last_page(check_urls[3])
get_last_page(check_urls[3])
get_last_page(check_urls[2])
get_last_page(check_urls[1])
get_last_page <- function(urls_to_check){
if(read_html(urls_to_check) %>%
html_nodes(".next_page") %>%
html_attrs() %>%
unlist() %>%
str_detect("disabled") %>%
any()
) {
urls_to_check
} else {
print("stopped")
}
}
get_last_page(check_urls[1])
get_last_page(check_urls)
lapply(check_urls, get_last_page)
source('H:/My Documents/R/Faculty Projects/course catalogue/workflow/src/initialise.R', echo=TRUE)
View(output_dataframe)
myapp <- oauth_app(appname = "dsca_course_catalogue",#app name
key = "902fce17b0920ced63c2",#app client ID
secret = "9dbb0b873f3b0241c136a913981434457f590a7c")#app secret
source('H:/My Documents/R/Faculty Projects/course catalogue/workflow/src/initialise.R', echo=TRUE)
source('git_ignore/api_credentials.R')
# Get OAuth credentials
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
# Use API
gtoken <- config(token = github_token)
req <- GET("https://api.github.com/users/jtleek/repos", gtoken)
req
req <- GET("https://api.github.com/users/r-leyshon/repos", gtoken)
req
# Extract content from a request
json1 = content(req)
json1
# Convert to a data.frame
gitDF = jsonlite::fromJSON(jsonlite::toJSON(json1))
View(gitDF)
gitDF
class(gitDF)
View(gitDF)
gtoken
req <- GET('https://api.github.com/search/repositories?q=tetris+language:assembly&sort=stars&order=desc',
gtoken,
user_agent('leyshonrr@hotmail.co.uk  testing access to search api'))
req
content(req)
req <- GET('https://api.github.com/search/org?q=datasciencecampus',
gtoken,
user_agent('leyshonrr@hotmail.co.uk  testing access to search api'))
req
content(req)
req <- GET('https://api.github.com/search/orgs?q=datasciencecampus',
gtoken,
user_agent('leyshonrr@hotmail.co.uk  testing access to search api'))
content(req)
req <- GET('https://api.github.com/search/users?q=datasciencecampus',
gtoken,
user_agent('leyshonrr@hotmail.co.uk  testing access to search api'))
content(req)
req_content <- content(req)
View(req_content)
req_content[["items"]][[1]][["url"]]
)
user_agent('richard.leyshon@ons.gov.uk  extracting repo metadata from employer organisation')
req <- GET("https://api.github.com/users/datasciencecampus",
gtoken,
user_agent('richard.leyshon@ons.gov.uk  extracting repo metadata from employer organisation')
)
req_content <- content(req)
req_content
req <- GET("https://api.github.com/users/datasciencecampus/repos",
gtoken,
user_agent('richard.leyshon@ons.gov.uk  extracting repo metadata from employer organisation')
)
req_content <- content(req)
req_content
request_result <- GET("https://api.github.com/users/datasciencecampus/repos",
gtoken,
user_agent('richard.leyshon@ons.gov.uk  extracting repo metadata from employer organisation')
)
# Check request_result
if(http_error(request_result)){
warning("The request failed")
} else {
content(request_result)
}
request_result <- GET("https://api.github.com/users/datasciencecampus/repos",
gtoken,
user_agent('richard.leyshon@ons.gov.uk  extracting repo metadata from employer organisation')
)
# Check request_result
if(http_error(request_result)){
warning("The request failed")
} else {
req_content <- content(request_result)
}
req_content
str(req_content)
class(req_content)
content(request_result, as = "text")
writeLines(content(request_result, as = "text"))
class(writeLines(content(request_result, as = "text")))
str(writeLines(content(request_result, as = "text")))
str(content(request_result, as = "text"))
content(request_result, as = "text")
content(request_result, max.level = 4)
content(request_result, max.level = 3)
content(request_result, max.level = 1)
str(content(request_result, max.level = 1))
str(content(request_result, max.level = 4))
req_content
req_content %>% html_nodes("html_url")
lapply(req_content, html_nodes)
source('H:/My Documents/R/Faculty Projects/course catalogue/workflow/src/initialise.R', echo=TRUE)
request_result <- GET("https://api.github.com/users/datasciencecampus/repos",
gtoken,
user_agent('richard.leyshon@ons.gov.uk  extracting repo metadata from employer organisation')
)
source('git_ignore/api_credentials.R')
# Get OAuth credentials
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
req <- GET('https://api.github.com/search/users?q=datasciencecampus',
gtoken,
user_agent('leyshonrr@hotmail.co.uk  testing access to search api'))
# Use API
gtoken <- config(token = github_token)
req <- GET('https://api.github.com/search/users?q=datasciencecampus',
gtoken,
user_agent('leyshonrr@hotmail.co.uk  testing access to search api'))
request_result <- GET("https://api.github.com/users/datasciencecampus/repos",
gtoken,
user_agent('richard.leyshon@ons.gov.uk  extracting repo metadata from employer organisation')
)
# Check request_result
if(http_error(request_result)){
warning("The request failed")
} else {
req_content <- content(request_result)
}
class(req_content)
str(writeLines(content(request_result, as = "text")))
content(request_result, as = "text")
str(content(request_result, max.level = 4))
test <- str(content(request_result, max.level = 4))
test <- content(request_result, max.level = 4)
View(test)
list.select(html_url)
list.select(req_content, html_url)
test1 <- list.select(req_content, html_url)
View(test1)
test2 <- list.stack(list.select(req_content, html_url))
View(test2)
source('H:/My Documents/R/Faculty Projects/course catalogue/workflow/src/initialise.R', echo=TRUE)
View(output_dataframe)
