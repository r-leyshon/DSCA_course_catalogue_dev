course_repo_names <- lapply(all_pages, extract_repo_names) %>% unlist()
install.packages("ProjectTemplate")
+=install.packages("rtools")
install.packages("rtools")
clear()
#load custom functions
source('functions/functions.R')
#load Github API credentials file
source('git_ignore/api_credentials.R')
source('~/dsca_course_catalogue_dev/src/initialise.R', echo=TRUE)
#load custom functions
source('functions/functions.R')
#load Github API credentials file
source('git_ignore/api_credentials.R')
# Get OAuth credentials
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
# Get OAuth credentials
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
# Use API
gtoken <- config(token = github_token)
request_result <- GET("https://api.github.com/users/datasciencecampus/repos",
gtoken,
user_agent('richard.leyshon@ons.gov.uk  extracting repo metadata from employer organisation')
)
#sleep in case of reruns & rate limit
Sys.sleep(1)
# Check request_result, print if error, extract content if none encountered
if(http_error(request_result)){
warning("The request failed")
} else {
req_content <- content(request_result)
}
all_pages <- list.stack(list.select(req_content, html_url))
all_pages <- all_pages[,1]
all_pages
print(paste("Page URL to scrape link from: ", all_pages))
print("There are", paste(length(all_pages),  "URLs to scrape link from: "))
print(paste("There are", length(all_pages),  "URLs to scrape link from: "))
print(all_pages)
#extract all course repo names from all_pages url vector
course_repo_names <- lapply(all_pages, extract_repo_names) %>% unlist()
View(extract_repo_names)
all_pages[1]
all_pages[1] %>% read_html()
all_pages[1] %>% read_html() %>% html_nodes("a")
all_pages[1] %>% read_html() %>% html_nodes("a") %>% html_attr("href")
hrefs <- all_pages[1] %>% read_html() %>% html_nodes("a") %>% html_attr("href")
cleansed_hrefs <- hrefs[!grepl("search", hrefs)]
cleansed_hrefs
hrefs
cleansed_hrefs
dsc_links <- cleansed_hrefs[grepl("/datasciencecampus/", cleansed_hrefs)]
dsc_links
course_repo_names <- unique(dsc_links[str_count(dsc_links, "/") == 2])
course_repo_names
course_repo_names <- unique(hrefs[str_count(hrefs, "/") == 2])
course_repo_names
repo_names_counted <- unique(hrefs[str_count(hrefs, "/") == 2])
repo_names <- repo_names_counted[grepl("/datasciencecampus/", repo_names_counted)]
repo_name <- repo_names_counted[grepl("/datasciencecampus/", repo_names_counted)]
repo_name
hrefs <- all_pages[2] %>% read_html() %>% html_nodes("a") %>% html_attr("href")
repo_names_counted <- unique(hrefs[str_count(hrefs, "/") == 2])
repo_name <- repo_names_counted[grepl("/datasciencecampus/", repo_names_counted)]
repo_name
hrefs <- all_pages[28] %>% read_html() %>% html_nodes("a") %>% html_attr("href")
repo_names_counted <- unique(hrefs[str_count(hrefs, "/") == 2])
repo_name <- repo_names_counted[grepl("/datasciencecampus/", repo_names_counted)]
repo_name
#purpose of script:
"Scrape data from online github DSC repo. This script handles course titles"
#load custom functions
source('functions/functions.R')
#load Github API credentials file
source('git_ignore/api_credentials.R')
# Get OAuth credentials
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
# Use API
gtoken <- config(token = github_token)
###################search api#####################
"30 requests per minute"
#request all repo content from datasciencecampus. Note that only public repos are detected.
request_result <- GET("https://api.github.com/users/datasciencecampus/repos",
gtoken,
user_agent('richard.leyshon@ons.gov.uk  extracting repo metadata from employer organisation')
)
#sleep in case of reruns & rate limit
Sys.sleep(1)
# Check request_result, print if error, extract content if none encountered
if(http_error(request_result)){
warning("The request failed")
} else {
req_content <- content(request_result)
}
all_pages <- list.stack(list.select(req_content, html_url))
"This method has identified 30 open rep addresses that could be used to build the address links object.
Consider using this method for identification as avoids the issue with identifying last page"
all_pages <- all_pages[,1]
print(paste("There are", length(all_pages),  "URLs to scrape link from: "))
print(all_pages)
################working here######################
################working here######################
################working here######################
#extract all course repo names from all_pages url vector
course_repo_names <- lapply(all_pages, extract_repo_names) %>% unlist()
course_repo_names
# verification messages to console
print(
paste("There are", length(course_repo_names), "courses found. Here are the repo names: ")
)
#messages to console
print(paste(course_repo_names))
output_dataframe <- data.frame(course_repo_names)
output_dataframe$course_repo_names <- str_remove(output_dataframe$course_repo_names, "/datasciencecampus/")
output_dataframe <- data.frame(course_repo_names)
#purpose of script:
"Scrape data from online github DSC repo. This script handles course titles"
#load custom functions
source('functions/functions.R')
#load Github API credentials file
source('git_ignore/api_credentials.R')
# Get OAuth credentials
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
# Use API
gtoken <- config(token = github_token)
###################search api#####################
"30 requests per minute"
#request all repo content from datasciencecampus. Note that only public repos are detected.
request_result <- GET("https://api.github.com/users/datasciencecampus/repos",
gtoken,
user_agent('richard.leyshon@ons.gov.uk  extracting repo metadata from employer organisation')
)
#sleep in case of reruns & rate limit
Sys.sleep(1)
# Check request_result, print if error, extract content if none encountered
if(http_error(request_result)){
warning("The request failed")
} else {
req_content <- content(request_result)
}
all_pages <- list.stack(list.select(req_content, html_url))
"This method has identified 30 open rep addresses that could be used to build the address links object.
Consider using this method for identification as avoids the issue with identifying last page"
all_pages <- all_pages[,1]
print(paste("There are", length(all_pages),  "URLs to scrape link from: "))
print(all_pages)
################working here######################
################working here######################
################working here######################
#extract all course repo names from all_pages url vector
course_repo_names <- lapply(all_pages, extract_repo_names) %>% unlist()
#need a generalised function to use in lapply.
# verification messages to console
print(
paste("There are", length(course_repo_names), "courses found. Here are the repo names: ")
)
output_dataframe$course_repo_names <- str_remove(output_dataframe$course_repo_names, "/datasciencecampus/")
course_repo_names <- str_remove(course_repo_names, "/datasciencecampus/")
course_repo_names
#messages to console
print(paste(course_repo_names))
# verification messages to console
print(
paste("There are", length(course_repo_names), "courses found. Here are the repo names: ")
)
#messages to console
print(paste(course_repo_names))
output_dataframe <- data.frame(course_repo_names)
#save these links as course names within the output dataframe
output_dataframe$course_location <- all_pages
all_pages
course_repo_names
grepl("skeletor", all_pages)
#purpose of script:
"Scrape data from online github DSC repo. This script handles course titles"
#load custom functions
source('functions/functions.R')
#load Github API credentials file
source('git_ignore/api_credentials.R')
# Get OAuth credentials
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
# Use API
gtoken <- config(token = github_token)
###################search api#####################
"30 requests per minute"
#request all repo content from datasciencecampus. Note that only public repos are detected.
request_result <- GET("https://api.github.com/users/datasciencecampus/repos",
gtoken,
user_agent('richard.leyshon@ons.gov.uk  extracting repo metadata from employer organisation')
)
#sleep in case of reruns & rate limit
Sys.sleep(1)
# Check request_result, print if error, extract content if none encountered
if(http_error(request_result)){
warning("The request failed")
} else {
req_content <- content(request_result)
}
all_pages <- list.stack(list.select(req_content, html_url))
"This method has identified 30 open rep addresses that could be used to build the address links object.
Consider using this method for identification as avoids the issue with identifying last page"
all_pages <- all_pages[,1]
print(paste("There are", length(all_pages),  "URLs to scrape link from: "))
print(all_pages)
################working here######################
################working here######################
################working here######################
#extract all course repo names from all_pages url vector
course_repo_names <- lapply(all_pages, extract_repo_names) %>% unlist()
course_repo_names
#purpose of script:
"Scrape data from online github DSC repo. This script handles course titles"
#load custom functions
source('functions/functions.R')
#load Github API credentials file
source('git_ignore/api_credentials.R')
# Get OAuth credentials
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
# Use API
gtoken <- config(token = github_token)
###################search api#####################
"30 requests per minute"
#request all repo content from datasciencecampus. Note that only public repos are detected.
request_result <- GET("https://api.github.com/users/datasciencecampus/repos",
gtoken,
user_agent('richard.leyshon@ons.gov.uk  extracting repo metadata from employer organisation')
)
#sleep in case of reruns & rate limit
Sys.sleep(1)
# Check request_result, print if error, extract content if none encountered
if(http_error(request_result)){
warning("The request failed")
} else {
req_content <- content(request_result)
}
all_pages <- list.stack(list.select(req_content, html_url))
"This method has identified 30 open rep addresses that could be used to build the address links object.
Consider using this method for identification as avoids the issue with identifying last page"
all_pages <- all_pages[,1]
print(paste("There are", length(all_pages),  "URLs to scrape link from: "))
print(all_pages)
################working here######################
################working here######################
################working here######################
#extract all course repo names from all_pages url vector
course_repo_names <- lapply(all_pages, extract_repo_names) %>% unlist()
course_repo_names
#extract all course repo names from all_pages url vector
course_repo_names <- unique(lapply(all_pages, extract_repo_names) %>% unlist())
course_repo_names
#cleanse the prefix from the repo name
course_repo_names <- str_remove(course_repo_names, "/datasciencecampus/")
# verification messages to console
print(
paste("There are", length(course_repo_names), "courses found. Here are the repo names: ")
)
# verification messages to console
print(
paste("There are", length(course_repo_names), "repos found. Here are the names: ")
)
#messages to console
print(paste(course_repo_names))
#initiate output dataframe
output_dataframe <- data.frame(course_repo_names)
#save these links as course names within the output dataframe
output_dataframe$course_location <- all_pages
install.packages("rowr")
https://www.rdocumentation.org/packages/rowr
https:///www.rdocumentation.org//packages//rowr
install.packages('rowr')
cbind.fill <- function(...){
nm <- list(...)
nm <- lapply(nm, as.matrix)
n <- max(sapply(nm, nrow))
do.call(cbind, lapply(nm, function (x)
rbind(x, matrix(, n-nrow(x), ncol(x)))))
}
cbind.fill(output_data, all_pages)
cbind.fill(output_dataframe, all_pages)
source('~/dsca_course_catalogue_dev/src/initialise.R', echo=TRUE)
source('~/dsca_course_catalogue_dev/munge/01_scrape_repo_names.R', echo=TRUE)
View(output_dataframe)
source('~/dsca_course_catalogue_dev/munge/01_scrape_repo_names.R', echo=TRUE)
?unique
source('~/dsca_course_catalogue_dev/munge/01_scrape_repo_names.R', echo=TRUE)
source('~/dsca_course_catalogue_dev/munge/01_scrape_repo_names.R', echo=TRUE)
names(output_dataframe)
class(output_dataframe)
#save these links as course names within the output dataframe
output_dataframe <- as.data.frame(cbind.fill(output_dataframe, all_pages))
#purpose of script:
"Scrape data from online github DSC repo. This script handles course titles"
#load custom functions
source('functions/functions.R')
#load Github API credentials file
source('git_ignore/api_credentials.R')
# Get OAuth credentials
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
# Use API
gtoken <- config(token = github_token)
###################search api#####################
"30 requests per minute"
#request all repo content from datasciencecampus. Note that only public repos are detected.
request_result <- GET("https://api.github.com/users/datasciencecampus/repos",
gtoken,
user_agent('richard.leyshon@ons.gov.uk  extracting repo metadata from employer organisation')
)
# Check request_result, print if error, extract content if none encountered
if(http_error(request_result)){
warning("The request failed")
} else {
req_content <- content(request_result)
}
#select all html_url items and stack as dataframe
all_pages <- list.stack(list.select(req_content, html_url))
"This method has identified 30 open rep addresses that could be used to build the address links object.
Consider using this method for identification as avoids the issue with identifying last page"
all_pages <- all_pages[,1]
print(paste("There are", length(all_pages),  "URLs to scrape link from: "))
print(all_pages)
#extract all course repo names from all_pages url vector
#must use unique as skeletor is appearing on two occasions for some reason, despite the skeletor
#public address not appearing in all_pages.
course_repo_names <- unique(lapply(all_pages, extract_repo_names) %>% unlist(), fromLast = TRUE)
#cleanse the prefix from the repo name
course_repo_names <- str_remove(course_repo_names, "/datasciencecampus/")
# verification messages to console
print(
paste("There are", length(course_repo_names), "repos found. Here are the names: ")
)
#messages to console
print(paste(course_repo_names))
#initiate output dataframe
output_dataframe <- data.frame(course_repo_names)
#save these links as course names within the output dataframe
output_dataframe <- as.data.frame(cbind.fill(output_dataframe, all_pages))
#rename output_dataframe to site link
names(output_dataframe)
names(output_dataframe)[2]
names(output_dataframe)[2] <- 'site_link'
names(output_dataframe)
is.na(output_dataframe$site_link)
output_dataframe <- mutate(
case_when(is.na(output_dataframe$site_link) ~ paste("https://github.com/datasciencecampus/", course_repo_names),
TRUE ~ output_dataframe$site_link
)
)
output_dataframe <- mutate(
case_when(is.na(output_dataframe$site_link) == TRUE ~ paste("https://github.com/datasciencecampus/", course_repo_names),
TRUE ~ output_dataframe$site_link
)
)
output_dataframe <- output_dataframe %>%
mutate(
case_when(is.na(site_link) == TRUE ~ paste("https://github.com/datasciencecampus/", course_repo_names),
TRUE ~ site_link
)
)
View(output_dataframe)
output_dataframe <- output_dataframe %>%
mutate(
case_when(is.na(site_link) == TRUE ~ 1),
TRUE ~ site_link
)
output_dataframe <- output_dataframe %>%
mutate(
case_when(is.na(site_link) == F ~ 1),
TRUE ~ site_link
)
output_dataframe <- output_dataframe %>%
mutate(
case_when(is.na(site_link) == TRUE ~ 1,
TRUE ~ site_link
)
)
output_dataframe$site_link[is.na(output_dataframe$site_link)]
is.na(output_dataframe$site_link)
output_dataframe$site_link[is.na(output_dataframe$site_link)] <- paste("https://github.com/datasciencecampus/", output_dataframe$course_repo_names)
output_dataframe <- output_dataframe %>%
mutate(
case_when(is.na(site_link) == TRUE ~ 1
)
)
output_dataframe <- output_dataframe %>%
mutate(
case_when(is.na(site_link) == TRUE ~ "Blah"
)
)
#purpose of script:
"Scrape data from online github DSC repo. This script handles course titles"
#load custom functions
source('functions/functions.R')
#load Github API credentials file
source('git_ignore/api_credentials.R')
# Get OAuth credentials
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
# Use API
gtoken <- config(token = github_token)
###################search api#####################
"30 requests per minute"
#request all repo content from datasciencecampus. Note that only public repos are detected.
request_result <- GET("https://api.github.com/users/datasciencecampus/repos",
gtoken,
user_agent('richard.leyshon@ons.gov.uk  extracting repo metadata from employer organisation')
)
# Check request_result, print if error, extract content if none encountered
if(http_error(request_result)){
warning("The request failed")
} else {
req_content <- content(request_result)
}
#select all html_url items and stack as dataframe
all_pages <- list.stack(list.select(req_content, html_url))
"This method has identified 30 open rep addresses that could be used to build the address links object.
Consider using this method for identification as avoids the issue with identifying last page"
all_pages <- all_pages[,1]
print(paste("There are", length(all_pages),  "URLs to scrape link from: "))
print(all_pages)
#extract all course repo names from all_pages url vector
#must use unique as skeletor is appearing on two occasions for some reason, despite the skeletor
#public address not appearing in all_pages.
course_repo_names <- unique(lapply(all_pages, extract_repo_names) %>% unlist(), fromLast = TRUE)
#cleanse the prefix from the repo name
course_repo_names <- str_remove(course_repo_names, "/datasciencecampus/")
# verification messages to console
print(
paste("There are", length(course_repo_names), "repos found. Here are the names: ")
)
#messages to console
print(paste(course_repo_names))
#initiate output dataframe
output_dataframe <- data.frame(course_repo_names)
#save these links as course names within the output dataframe
output_dataframe <- as.data.frame(cbind.fill(output_dataframe, all_pages))
#rename output_dataframe to site link
names(output_dataframe)[2] <- 'site_link'
"In the case of missing course addresses for identified repo names (ref: skeletor-public),
we can construct the required url"
output_dataframe <- output_dataframe %>%
mutate(
case_when(is.na(site_link) == TRUE ~ "Blah"
)
)
output_dataframe$site_link[is.na(output_dataframe$site_link)] <- paste("https://github.com/datasciencecampus/", output_dataframe$course_repo_names[is.na(output_dataframe$site_link)])
#select all html_url items and stack as dataframe
all_pages <- list.stack(list.select(req_content, html_url))
"This method has identified 30 open rep addresses that could be used to build the address links object.
Consider using this method for identification as avoids the issue with identifying last page"
all_pages <- all_pages[,1]
print(paste("There are", length(all_pages),  "URLs to scrape link from: "))
print(all_pages)
#extract all course repo names from all_pages url vector
#must use unique as skeletor is appearing on two occasions for some reason, despite the skeletor
#public address not appearing in all_pages.
course_repo_names <- unique(lapply(all_pages, extract_repo_names) %>% unlist(), fromLast = TRUE)
#cleanse the prefix from the repo name
course_repo_names <- str_remove(course_repo_names, "/datasciencecampus/")
# verification messages to console
print(
paste("There are", length(course_repo_names), "repos found. Here are the names: ")
)
#messages to console
print(paste(course_repo_names))
#initiate output dataframe
output_dataframe <- data.frame(course_repo_names)
#save these links as course names within the output dataframe
output_dataframe <- as.data.frame(cbind.fill(output_dataframe, all_pages))
#rename output_dataframe to site link
names(output_dataframe)[2] <- 'site_link'
"In the case of missing course addresses for identified repo names (ref: skeletor-public),
we can construct the required url"
output_dataframe$site_link[is.na(output_dataframe$site_link)] <- paste0("https://github.com/datasciencecampus/", output_dataframe$course_repo_names[is.na(output_dataframe$site_link)])
#extract all course repo names from all_pages url vector
mr3_cleansed <- lapply(all_pages, extract_mr3) %>% unlist()
mr3_cleansed
all_pages
#extract all course repo names from all_pages url vector
mr3_cleansed <- lapply(all_pages, extract_mr3) %>% unlist()
mr3_cleansed
all_pages %>% read_html() %>%  xml_find_all(xpath = '//*[contains(concat( " ", @class, " " ), concat( " ", "mr-3", " " ))]')
all_pages[1] %>% read_html() %>%  xml_find_all(xpath = '//*[contains(concat( " ", @class, " " ), concat( " ", "mr-3", " " ))]')
all_pages[1] %>% read_html() %>%  xml_find_all(xpath = '//*[contains(concat( " ", @class, " " ), concat( " ", "mr-3", " " ))]') %>%
html_text()
all_pages[1]
source('~/dsca_course_catalogue_dev/src/initialise.R', echo=TRUE)
source('~/dsca_course_catalogue_dev/src/initialise.R', echo=TRUE)
View(output_dataframe)
parsed_course_pages <- lapply(output_dataframe$site_link, read_html)
parsed_course_pages
source('~/dsca_course_catalogue_dev/src/initialise.R', echo=TRUE)
View(output_dataframe)
View(output_dataframe)
source('~/dsca_course_catalogue_dev/src/initialise.R', echo=TRUE)
source('~/dsca_course_catalogue_dev/src/initialise.R', echo=TRUE)
View(output_dataframe)
install.packages("xlsx")
#write data to file
xlsx::write.xlsx(output_dataframe, "output_data/course_catalogue.xlsx",
sheetName = paste(Sys.Date()),
row.names = FALSE)
install.packages("rjava")
install.packages("rJava")
#write data to file
xlsx::write.xlsx(output_dataframe, "output_data/course_catalogue.xlsx",
sheetName = paste(Sys.Date()),
row.names = FALSE)
library(xlsx)
library(xlsx)
#write data to file
xlsx::write.xlsx(output_dataframe, "output_data/course_catalogue.xlsx",
sheetName = paste(Sys.Date()),
row.names = FALSE)
